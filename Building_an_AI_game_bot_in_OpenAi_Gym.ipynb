{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building an AI game bot in OpenAi Gym.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nunival/462-Computer-Vision/blob/main/Building_an_AI_game_bot_in_OpenAi_Gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efr929BAmzc"
      },
      "source": [
        "# Building an AI Game Bot\n",
        "***\n",
        "In this notebook I attempt to solve the MountainCar game in OpenAI Gym.<br>\n",
        "<i>\n",
        "\"A car is on a one-dimensional track, positioned between two \"mountains\". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\"\n",
        "</i>\n",
        "<br>[Source](https://gym.openai.com/envs/MountainCar-v0/)\n",
        "\n",
        "My approach for solving this is to train a TensorFlow model based on data from game play. This model will tell the car what to do based on the observations.\n",
        "\n",
        "The data for this model will be generated by running 10,000 games with random moves. Based on the score of the game we can train the model to see which are good or bad moves.\n",
        "\n",
        "\n",
        "**Resources:** \n",
        "* [Link to details on MountainCar game](https://github.com/openai/gym/wiki/MountainCar-v0)\n",
        "* [This blog post](https://blog.tanka.la/2018/10/19/build-your-first-ai-game-bot-using-openai-gym-keras-tensorflow-in-python/) uses this approach to solve the CartPole game. I attempt the same approach on MountainCar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2"
      },
      "source": [
        "# Install dependancies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APXSx7hg19TH"
      },
      "source": [
        "# Imports and Helper functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3BGbWOu179M"
      },
      "source": [
        "# Mountain Car"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGEFMfDOzLen"
      },
      "source": [
        "env = wrap_env(gym.make(\"MountainCar-v0\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BmIlXhe9Q89"
      },
      "source": [
        "#check out the action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMj98XmBbXu"
      },
      "source": [
        "There are 3 actions the computer can take:<br>\n",
        "0 - Go left<br>\n",
        "1 - Do nothing<br>\n",
        "2 - Go right"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9RIDvroBr1b"
      },
      "source": [
        "## Understanding the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dDeBxv4CVYk"
      },
      "source": [
        "Viewing a game where random actions are taken"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ1JB6Uu4bA3"
      },
      "source": [
        "env = wrap_env(gym.make(\"MountainCar-v0\"))\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample()\n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTshlWObC4Bz"
      },
      "source": [
        "### Viewing the data that the game produces\n",
        "We see that each observation has two data points: position and velocity. We can also see the reward and what action was taken (go left/right)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74kljixF9Etb"
      },
      "source": [
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    #print(\"Step {}:\".format(step_index))\n",
        "    print(\"action: {}\".format(action))\n",
        "    print(\"reward: {}\".format(reward))\n",
        "    print(\"done: {}\".format(done))\n",
        "    print(\"info: {}\".format(info))\n",
        "    print(\"observation: {}\".format(observation))\n",
        "    print(\"***********************************************************\")\n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct_GD4AtB1SQ"
      },
      "source": [
        "## Creating Data for a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-oE6AIU7U8L"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.models     import Sequential\n",
        "from keras.layers     import Dense\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGX13fd1DKO6"
      },
      "source": [
        "To train a model we need data. We'll run 10,000 (initial_games) games to generate data. We'll save this data and use it to teach a model which actions are good and which aren't."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNpkI-opvrks"
      },
      "source": [
        "env.reset()\n",
        "goal_steps = 200            # Game ends after 200 moves\n",
        "\n",
        "# Read the documentation at the top, for each move we lose a point. So \n",
        "# we only want games that do better than -200 (that means the game never\n",
        "# won)\n",
        "score_requirement = -199    \n",
        "\n",
        "# We're going to run 10,000 games\n",
        "intial_games = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBY0Y-ZdvvrR"
      },
      "source": [
        "def model_data_preparation():\n",
        "    training_data = []\n",
        "    accepted_scores = []\n",
        "    for game_index in range(intial_games):\n",
        "        score = 0\n",
        "        game_memory = []\n",
        "        previous_observation = []\n",
        "        for step_index in range(goal_steps):\n",
        "            action = random.randrange(0, 3) #Pick a random move: 0, 1, 2\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            \n",
        "            if len(previous_observation) > 0:\n",
        "                game_memory.append([previous_observation, action])\n",
        "                \n",
        "            previous_observation = observation\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "        if score >= score_requirement:\n",
        "            accepted_scores.append(score)\n",
        "            for data in game_memory:\n",
        "                if data[1] == 1:        # One hot encoding the move\n",
        "                    output = [0, 1, 0]\n",
        "                elif data[1] == 0:\n",
        "                    output = [1, 0, 0]\n",
        "                elif data[1] == 2:\n",
        "                    output = [0, 0, 1]\n",
        "                training_data.append([data[0], output])\n",
        "        if game_index % 100 == 0:             # allows us to monitor how many games have progressed\n",
        "          print(\"Games Completed: \", game_index)\n",
        "        env.reset()\n",
        "\n",
        "    print(accepted_scores)\n",
        "    \n",
        "    return training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfUBlBQYwY5w"
      },
      "source": [
        "training_data = model_data_preparation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr8WoaxPtfzP"
      },
      "source": [
        "len(training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJsVJOcPEAcw"
      },
      "source": [
        "So we have a problem here. Not a single one of our games wins based on random moves. All of our games have a -200 score (since they're not winning they take all 200 moves and get -1 score per move).\n",
        "\n",
        "With all of our games having the same score we can't teach a model which choices to make, they're all equally bad. \n",
        "\n",
        "This code will need to be tweaked to find a different reward. Perhaps I can append the position and use that as the indicator of success. For example the highest position achieved per game. This could be used to identify which games performed better than others, even if none of them won."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5WUKDcQv-sG"
      },
      "source": [
        "# Building a sequential model\n",
        "\n",
        "def build_model(input_size, output_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_size, activation='relu'))\n",
        "    model.add(Dense(52, activation='relu'))\n",
        "    model.add(Dense(output_size, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam())\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osWm8DexwDdo"
      },
      "source": [
        "# Training the model \n",
        "\n",
        "def train_model(training_data):\n",
        "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))\n",
        "    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))\n",
        "    model = build_model(input_size=len(X[0]), output_size=len(y[0]))\n",
        "    \n",
        "    model.fit(X, y, epochs=10)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUxvxtXBwM1C"
      },
      "source": [
        "trained_model = train_model(training_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rigiucb9EL4e"
      },
      "source": [
        "Below we'll run 100 games using the model to make the decision on which action to take. We then at the end aggregate the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5MDTlulwOun"
      },
      "source": [
        "scores = []\n",
        "choices = []\n",
        "for each_game in range(100):\n",
        "    score = 0\n",
        "    prev_obs = []\n",
        "    for step_index in range(goal_steps):\n",
        "         env.render()\n",
        "         if len(prev_obs)==0:\n",
        "           action = random.randrange(0,3)\n",
        "         else:\n",
        "           action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])\n",
        "        \n",
        "         choices.append(action)\n",
        "         new_observation, reward, done, info = env.step(action)\n",
        "         prev_obs = new_observation\n",
        "         score+=reward\n",
        "         if done:\n",
        "             break\n",
        "\n",
        "    env.reset()\n",
        "    scores.append(score)\n",
        "\n",
        "print(scores)\n",
        "print('Average Score:', sum(scores)/len(scores))\n",
        "print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S264IdMXE7Fb"
      },
      "source": [
        "# A Simpler Solution\n",
        "\n",
        "So as noted above the process breaks down because of the reward structure. There is no way to gain points, only try to lose as few as possible by winning as soon as possible. \n",
        "\n",
        "I took a step back and thought about this problem. If I were playing the game, not the computer, how would I play? Well I would go up one hill and as soon as I started going back down I'd hit the key to go in that direction. \n",
        "\n",
        "It occured to me that this logic could be very easily written. If the car is going to the left drive to the left, if it's going to the right drive to the right. \n",
        "\n",
        "As you can see below I use 4 lines of code to win the game. \n",
        "\n",
        "Now it isn't transferable, this code won't work for any game. But if the goal is to solve the game sometimes a fancy complicated Deep Learning model isn't necessary. While I still plan to play around with the TensorFlow approach it's a good reminder. Sometimes the simplest solution is the best."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5gZ1ZdI1OfC"
      },
      "source": [
        "env = wrap_env(gym.make(\"MountainCar-v0\"))\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    \n",
        "    if observation[1] < 0 : # If Velocity is negative go left\n",
        "      action = 0\n",
        "    else:                   # Else go right\n",
        "      action = 2\n",
        "\n",
        "    observation, reward, done, info = env.step(action) \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()\n",
        "print(\"Reward: \", reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnVqXEMjF7ri"
      },
      "source": [
        "# Victory!"
      ]
    }
  ]
}